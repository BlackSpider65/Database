{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbd06af",
   "metadata": {},
   "source": [
    "### Question No :- 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90d952",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of extracting information from websites. It involves fetching and parsing HTML content from web pages using tools or libraries in programming languages like Python. Web scraping allows automated data extraction from various websites, helping to gather data that can be used for analysis, research, or other purposes.\n",
    "\n",
    "\n",
    "Here are three areas where web scraping is commonly used to obtain data:-\n",
    "\n",
    "\n",
    "1) Business and Market Research:- Companies use web scraping to collect data on competitors, market trends, pricing information, customer reviews, and product details from various e-commerce websites. This data aids in making informed business decisions, understanding consumer behavior, and analyzing market trends.\n",
    "\n",
    "\n",
    "2) Data Analytics and Research:- Web scraping is utilized in data analytics and academic research to gather information from diverse sources on the internet. It helps in collecting data for sentiment analysis, social media monitoring, academic research papers, and building datasets for machine learning models.\n",
    "\n",
    "\n",
    "3) Real Estate and Property Listings:- Real estate companies and individuals involved in property investment use web scraping to extract data from real estate websites. This data includes property listings, pricing, location details, and historical sales data, facilitating better decision-making in property investments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1d840",
   "metadata": {},
   "source": [
    "### Question No :- 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66bb8f",
   "metadata": {},
   "source": [
    "There are various methods and techniques used for web scraping in Python, each with its own advantages and suitability for different scenarios. Some of the common methods include-\n",
    "\n",
    "1) Using Requests and Beautiful Soup:- This method involves using the Requests library to fetch HTML content from web pages and then utilizing Beautiful Soup to parse the HTML and extract desired information. It's effective for simple scraping tasks and provides flexibility in navigating and extracting data from the HTML structure.\n",
    "\n",
    "2) Selenium:- Selenium is a powerful tool mainly used for web automation and testing. It can also be used for scraping dynamic content rendered by JavaScript. It automates a web browser to interact with web elements, allowing access to content that may require user interaction or dynamic loading.\n",
    "\n",
    "3) Scrapy:- Scrapy is a comprehensive web crawling and scraping framework. It provides a complete toolset for creating web spiders that navigate websites, extract data, and store it in various formats. Scrapy allows more structured and scalable scraping solutions, suitable for large-scale projects.\n",
    "\n",
    "4) APIs:- Some websites offer APIs (Application Programming Interfaces) that allow access to their data in a structured format. Using APIs for data retrieval is often more reliable, legal, and encouraged compared to scraping HTML directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c56f7a",
   "metadata": {},
   "source": [
    "### Question No :-03 \n",
    "\n",
    "### What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9dfa9",
   "metadata": {},
   "source": [
    "Beautiful Soup is a popular Python library used for parsing HTML and XML documents. It is primarily used for web scraping purposes, allowing developers to extract data from web pages in a simple and effective manner.\n",
    "\n",
    "\n",
    "The main purposes and functionalities of Beautiful Soup include:\n",
    "\n",
    "\n",
    "1) Parsing HTML and XML:- Beautiful Soup helps in parsing HTML and XML documents by creating a parse tree that allows easy navigation and search operations within the document's structure. It provides methods to parse the document and extract desired information by traversing the parsed tree.\n",
    "\n",
    "2) Data Extraction:- It provides a convenient way to extract data from HTML documents by allowing users to find elements by tag name, class, ID, or other attributes. Once identified, data can be extracted from these elements, such as text, links, tables, or specific attributes.\n",
    "\n",
    "3) Handling Broken HTML:- Beautiful Soup is capable of handling imperfect or poorly formatted HTML. It can parse and work with HTML documents that might have errors or inconsistencies, making it robust for scraping data from various websites.\n",
    "\n",
    "4) Navigating the Parse Tree:- It offers methods to navigate the parse tree effortlessly. Users can move up and down the tree structure, access parent, child, and sibling elements, making it easy to locate specific data within the HTML document.\n",
    "\n",
    "5) Integration with Different Parsers:- Beautiful Soup supports different parsers, such as Python's built-in parsers like html.parser, as well as external parsers like lxml and html5lib. Users can choose the parser based on performance, speed, or parsing requirements.\n",
    "\n",
    "6) Ease of Use:- It provides a user-friendly interface and simple syntax, making it accessible for beginners while offering powerful features for advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c72bf6",
   "metadata": {},
   "source": [
    "### Question No :- 04\n",
    "\n",
    "### Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf571d2",
   "metadata": {},
   "source": [
    "Flask is a lightweight and versatile web framework for Python that is commonly used to build web applications. While it's not specifically designed for web scraping, Flask can be used in conjunction with web scraping projects for several reasons:-\n",
    "\n",
    "\n",
    "1) Building Web Interfaces:- Flask allows you to create web interfaces or dashboards that interact with your scraped data. You can display the scraped data in a user-friendly format, perform analysis, and present it to users through a web application.\n",
    "\n",
    "2) API Development:- Flask can be used to create APIs that serve the scraped data to other applications or services. Instead of directly scraping data on request, you can scrape it periodically and provide access to this data via endpoints created using Flask routes.\n",
    "\n",
    "3) Data Visualization and Reporting:- Flask can integrate with various visualization libraries and frameworks like Plotly, Bokeh, or Matplotlib, allowing you to create graphs, charts, and reports based on the scraped data. This can be beneficial for data analysis and presentation purposes.\n",
    "\n",
    "4) Job Scheduling and Automation:- Flask extensions such as Flask-Scheduler or Flask-Celery can help in scheduling and automating the scraping tasks. You can set up periodic scraping jobs or asynchronous tasks to gather data at specific intervals.\n",
    "\n",
    "5) Frontend Development:- Flask can serve as the backend for a web scraping project while allowing frontend technologies like HTML, CSS, and JavaScript to be used for creating the user interface. This separation of concerns enables efficient development and maintenance.\n",
    "\n",
    "6) Integration with Databases:- Flask can be integrated with various databases (e.g., SQLite, MySQL, PostgreSQL) using SQLAlchemy or other ORM (Object-Relational Mapping) libraries. This allows you to store and manage the scraped data in a structured manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fd108",
   "metadata": {},
   "source": [
    "### Question No :- 05\n",
    "\n",
    "###  Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ce20c",
   "metadata": {},
   "source": [
    "In a web scraping project hosted on AWS (Amazon Web Services), several services might be used to facilitate various aspects of the project, including hosting, storage, computation, and more.\n",
    "\n",
    "The name of AWS services used in project is Amazon EC2 (Elastic Compute Cloud). EC2 provides scalable compute capacity in the cloud. It can be used to host web scraping scripts or applications. You can run Python scripts or applications that perform web scraping tasks on EC2 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf31f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
